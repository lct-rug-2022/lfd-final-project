{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["-MOYcHm6ZmlZ","OrFPX32s1IDJ","OcRgEMuKlRxN","HngQriU62HTL","uPuN3jUD2LJV"],"authorship_tag":"ABX9TyO5lHsZ9ywPwXHbca6fVqWN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"ef37634bdab4455098e7d6874895cb7d":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_3c17889ea8bf4b13b5b2eb83a52005a0","IPY_MODEL_66f97a031e924996ad24bd05a5ed7447","IPY_MODEL_aedd5c847893460c9487fd01e2cd76df","IPY_MODEL_2d4ca45ff6cb4bc18b37ed4a3999c8cd"],"layout":"IPY_MODEL_fadd6dd2a0eb45e496b6cdbaa976a916"}},"3c17889ea8bf4b13b5b2eb83a52005a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8f477197cc840f7b95ad07a4883c611","placeholder":"​","style":"IPY_MODEL_4e3efc7096d7458685005962c98dcb50","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"66f97a031e924996ad24bd05a5ed7447":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_0511e7ad1f624c2b98b4e0010c3c10ff","placeholder":"​","style":"IPY_MODEL_576b582dc9dd4defae135223bdd1130f","value":""}},"aedd5c847893460c9487fd01e2cd76df":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_2a13182900a94fa693308b3bf876ff02","style":"IPY_MODEL_27d8ac8ce8fa472e98be235187b50498","tooltip":""}},"2d4ca45ff6cb4bc18b37ed4a3999c8cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d358d4c5304c4bc4b2da16228f5fb857","placeholder":"​","style":"IPY_MODEL_c146707169a24266bbb0f98dd2227e58","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"fadd6dd2a0eb45e496b6cdbaa976a916":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"d8f477197cc840f7b95ad07a4883c611":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e3efc7096d7458685005962c98dcb50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0511e7ad1f624c2b98b4e0010c3c10ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"576b582dc9dd4defae135223bdd1130f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a13182900a94fa693308b3bf876ff02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27d8ac8ce8fa472e98be235187b50498":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"d358d4c5304c4bc4b2da16228f5fb857":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c146707169a24266bbb0f98dd2227e58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8460cc808bc42bba9d7440c3462bb6b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_09f4f73272df4ace9a4fb2dd178f1508","IPY_MODEL_dc42446d0ac040678ed8fdb0c8fa29c9","IPY_MODEL_b6544f55065d41f0a5d30aee70ae9008"],"layout":"IPY_MODEL_a6a3bd249d4643aa829824dc183598d2"}},"09f4f73272df4ace9a4fb2dd178f1508":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ca0b141ae094ace8af6b21c6479b72b","placeholder":"​","style":"IPY_MODEL_e5e924d1fc7747a4a1895cf5a844b141","value":"100%"}},"dc42446d0ac040678ed8fdb0c8fa29c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d5f17f9810a41f292ec33a1c29c7e2c","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8cb9c186c33744eca5a7d566813a1b19","value":3}},"b6544f55065d41f0a5d30aee70ae9008":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb373f4946e847668cf786141da4fdaf","placeholder":"​","style":"IPY_MODEL_be4c0a5193c44db99a0dd5a17b17fed5","value":" 3/3 [00:00&lt;00:00, 75.12it/s]"}},"a6a3bd249d4643aa829824dc183598d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ca0b141ae094ace8af6b21c6479b72b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5e924d1fc7747a4a1895cf5a844b141":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d5f17f9810a41f292ec33a1c29c7e2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cb9c186c33744eca5a7d566813a1b19":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb373f4946e847668cf786141da4fdaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be4c0a5193c44db99a0dd5a17b17fed5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["#In this notebook we load our fine-tuned BERT basedmodels and evaluate them on the validation and test sets"],"metadata":{"id":"tBbhF7OK07mX"}},{"cell_type":"markdown","source":["# Install"],"metadata":{"id":"-MOYcHm6ZmlZ"}},{"cell_type":"code","source":["!pip install -U -q datasets transformers torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7cTb-jmZqqO","executionInfo":{"status":"ok","timestamp":1667125192550,"user_tz":-60,"elapsed":15759,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"bf3245a9-ddcf-42ac-cd58-8a5c3f56c784"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 441 kB 19.4 MB/s \n","\u001b[K     |████████████████████████████████| 5.3 MB 40.3 MB/s \n","\u001b[K     |████████████████████████████████| 163 kB 73.2 MB/s \n","\u001b[K     |████████████████████████████████| 115 kB 54.7 MB/s \n","\u001b[K     |████████████████████████████████| 212 kB 31.8 MB/s \n","\u001b[K     |████████████████████████████████| 127 kB 70.6 MB/s \n","\u001b[K     |████████████████████████████████| 7.6 MB 55.4 MB/s \n","\u001b[K     |████████████████████████████████| 115 kB 74.4 MB/s \n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"eKtC39EyZsLq"}},{"cell_type":"code","source":["from datasets import Dataset, DatasetDict, ClassLabel, Value, load_dataset, load_metric\n","from huggingface_hub import notebook_login\n","from torchinfo import summary\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n","import pandas as pd\n","from sklearn.metrics import classification_report"],"metadata":{"id":"03o3evuEZtZ3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data loading"],"metadata":{"id":"OrFPX32s1IDJ"}},{"cell_type":"code","source":["# login to HF\n","notebook_login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299,"referenced_widgets":["ef37634bdab4455098e7d6874895cb7d","3c17889ea8bf4b13b5b2eb83a52005a0","66f97a031e924996ad24bd05a5ed7447","aedd5c847893460c9487fd01e2cd76df","2d4ca45ff6cb4bc18b37ed4a3999c8cd","fadd6dd2a0eb45e496b6cdbaa976a916","d8f477197cc840f7b95ad07a4883c611","4e3efc7096d7458685005962c98dcb50","0511e7ad1f624c2b98b4e0010c3c10ff","576b582dc9dd4defae135223bdd1130f","2a13182900a94fa693308b3bf876ff02","27d8ac8ce8fa472e98be235187b50498","d358d4c5304c4bc4b2da16228f5fb857","c146707169a24266bbb0f98dd2227e58"]},"id":"wb9WAqElZ6Aj","executionInfo":{"status":"ok","timestamp":1667125232215,"user_tz":-60,"elapsed":460,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"abc193be-738b-4c12-9a54-f0229bc7cc96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Login successful\n","Your token has been saved to /root/.huggingface/token\n"]}]},{"cell_type":"code","source":["# load dataset\n","ds = load_dataset('sara-nabhani/lfd-proj',\n","    'csv', \n","    data_files={'train': 'train.csv', 'val': 'val.csv', 'test': 'test.csv'}\n",")\n","\n","cl = ClassLabel(names=list(ds['train'].unique('label')))\n","ds = ds.cast_column('label', cl)\n","\n","ds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399,"referenced_widgets":["c8460cc808bc42bba9d7440c3462bb6b","09f4f73272df4ace9a4fb2dd178f1508","dc42446d0ac040678ed8fdb0c8fa29c9","b6544f55065d41f0a5d30aee70ae9008","a6a3bd249d4643aa829824dc183598d2","1ca0b141ae094ace8af6b21c6479b72b","e5e924d1fc7747a4a1895cf5a844b141","3d5f17f9810a41f292ec33a1c29c7e2c","8cb9c186c33744eca5a7d566813a1b19","cb373f4946e847668cf786141da4fdaf","be4c0a5193c44db99a0dd5a17b17fed5"]},"id":"tPeaaR7vZ6x7","executionInfo":{"status":"ok","timestamp":1667130142256,"user_tz":-60,"elapsed":5188,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"c626e164-c817-4777-ac77-ab6d05a78c21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Using custom data configuration sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0\n","WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8460cc808bc42bba9d7440c3462bb6b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a79fd22edd8d6545.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4ab4ddb9d3857fe8.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-934869a6ab0c2224.arrow\n"]},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['tweet', 'label'],\n","        num_rows: 12240\n","    })\n","    val: Dataset({\n","        features: ['tweet', 'label'],\n","        num_rows: 1000\n","    })\n","    test: Dataset({\n","        features: ['tweet', 'label'],\n","        num_rows: 860\n","    })\n","})"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","source":["# bert-base-cased-finetuned-0 Model Evaluation"],"metadata":{"id":"OcRgEMuKlRxN"}},{"cell_type":"code","source":["model_id = 'sara-nabhani/bert-base-cased-fintuned-0'\n","#load tokenizer and tokenize dataset\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples['tweet'], padding='max_length', truncation=True)\n","\n","tokenized_ds = ds.map(tokenize_function, batched=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZLHLbCYaLtz","executionInfo":{"status":"ok","timestamp":1667130143048,"user_tz":-60,"elapsed":798,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"ccd676b8-ef78-4370-f4dd-4a96141daf3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-0/snapshots/e81c7ea00e68bb25cc60943bf7674a8c2d0a9f27/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-0/snapshots/e81c7ea00e68bb25cc60943bf7674a8c2d0a9f27/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-0/snapshots/e81c7ea00e68bb25cc60943bf7674a8c2d0a9f27/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-0/snapshots/e81c7ea00e68bb25cc60943bf7674a8c2d0a9f27/tokenizer_config.json\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-72488f28853a73e8.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f4cfb7b92b8a3443.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5274fe1bcb5ebb72.arrow\n"]}]},{"cell_type":"code","source":["metric = load_metric(\"accuracy\")\n","metric1= load_metric(\"f1\")\n","# evaluation function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return {\"acc\": metric.compute(predictions=predictions, references=labels), \"f1\":metric1.compute(predictions=predictions, references=labels, average='macro')}\n","#load model\n","model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n","trainer = Trainer(\n","  model=model,\n","  train_dataset=tokenized_ds['train'],\n","  eval_dataset=tokenized_ds['val'],\n","  compute_metrics=compute_metrics,\n","  tokenizer=tokenizer,\n","  data_collator=data_collator\n","  )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-ROpfOOap7Z","executionInfo":{"status":"ok","timestamp":1667130148203,"user_tz":-60,"elapsed":4319,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"da4234a2-3497-4297-a277-ad3e19bf36df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-0/snapshots/e81c7ea00e68bb25cc60943bf7674a8c2d0a9f27/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sara-nabhani/bert-base-cased-fintuned-0\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"OFF\",\n","    \"1\": \"NOT\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"NOT\": 1,\n","    \"OFF\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.23.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-0/snapshots/e81c7ea00e68bb25cc60943bf7674a8c2d0a9f27/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sara-nabhani/bert-base-cased-fintuned-0.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"HngQriU62HTL"}},{"cell_type":"code","source":["print(\"Prediction on validation data:\")\n","print(trainer.predict(tokenized_ds['val'])[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"id":"1baF_acra6nS","executionInfo":{"status":"ok","timestamp":1667130186018,"user_tz":-60,"elapsed":35815,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"d6d60cda-5713-43b8-d8e0-e1467d2ec504"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1000\n","  Batch size = 8\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["Prediction on validation data:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'test_loss': 0.4839080274105072, 'test_acc': {'accuracy': 0.79}, 'test_f1': {'f1': 0.7686164586418447}, 'test_runtime': 35.2114, 'test_samples_per_second': 28.4, 'test_steps_per_second': 3.55}\n"]}]},{"cell_type":"code","source":["print(\"Prediction on test data:\")\n","print(trainer.predict(tokenized_ds['test'])[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"3TqY6LS1byGd","executionInfo":{"status":"ok","timestamp":1667130215918,"user_tz":-60,"elapsed":29922,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"96e73c85-95b0-463e-d448-065f0efd8b09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 860\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["Prediction on test data:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'test_loss': 0.3805697560310364, 'test_acc': {'accuracy': 0.8511627906976744}, 'test_f1': {'f1': 0.8073840769903763}, 'test_runtime': 30.2254, 'test_samples_per_second': 28.453, 'test_steps_per_second': 3.573}\n"]}]},{"cell_type":"markdown","source":["## Saving to DF"],"metadata":{"id":"uPuN3jUD2LJV"}},{"cell_type":"code","source":["val_predictions = np.argmax(trainer.predict(tokenized_ds['val'])[0], axis=-1)\n","test_predictions = np.argmax(trainer.predict(tokenized_ds['test'])[0], axis=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"p-mP_Zjhb16X","executionInfo":{"status":"ok","timestamp":1667130280637,"user_tz":-60,"elapsed":64727,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"040659a5-ca5b-4476-f52a-3b530927f0c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1000\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 860\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"code","source":["val_data_frame=pd.DataFrame({'tweet': ds['val']['tweet'], 'true_label': ds['val']['label'], 'pred_label': val_predictions})\n","test_data_frame=pd.DataFrame({'tweet': ds['test']['tweet'], 'true_label': ds['test']['label'], 'pred_label': test_predictions})"],"metadata":{"id":"1nE9sluCcMZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Validation classification report:\")\n","print(classification_report(val_data_frame['true_label'], val_data_frame['pred_label']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onmr6M9mcb6d","executionInfo":{"status":"ok","timestamp":1667130280638,"user_tz":-60,"elapsed":45,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"be6bc5c8-b6ba-416b-8bc0-112ebf9113b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation classification report:\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.69      0.70       352\n","           1       0.83      0.84      0.84       648\n","\n","    accuracy                           0.79      1000\n","   macro avg       0.77      0.77      0.77      1000\n","weighted avg       0.79      0.79      0.79      1000\n","\n"]}]},{"cell_type":"code","source":["print(\"Test classification report:\")\n","print(classification_report(test_data_frame['true_label'], test_data_frame['pred_label']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4NGJd02KdU9Z","executionInfo":{"status":"ok","timestamp":1667130280638,"user_tz":-60,"elapsed":42,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"179253c4-c3ca-4a4a-e24a-cd077385e71e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test classification report:\n","              precision    recall  f1-score   support\n","\n","           0       0.77      0.67      0.72       240\n","           1       0.88      0.92      0.90       620\n","\n","    accuracy                           0.85       860\n","   macro avg       0.82      0.80      0.81       860\n","weighted avg       0.85      0.85      0.85       860\n","\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWoy8szmhn1O","executionInfo":{"status":"ok","timestamp":1667128310111,"user_tz":-60,"elapsed":59930,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"7b24f673-2bb7-4965-f739-3ab2007e5c48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["test_data_frame.to_csv('/content/gdrive/MyDrive/lfd-proj/models-output/bert-base-cased-finetuned-0.csv', index=False)\n","val_data_frame.to_csv('/content/gdrive/MyDrive/lfd-proj/val-models-output/bert-base-cased-finetuned-0.csv', index=False)"],"metadata":{"id":"0i5mUztEk4k7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1eEs6rCnl3O7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# bert-base-cased-finetuned-1 Model Evaluation"],"metadata":{"id":"nZI7JurwmNkX"}},{"cell_type":"code","source":["model_id = 'sara-nabhani/bert-base-cased-fintuned-1'\n","#load tokenizer and tokenize dataset\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples['tweet'], padding='max_length', truncation=True)\n","\n","tokenized_ds = ds.map(tokenize_function, batched=True)\n","\n","print(len(tokenized_ds['train']['input_ids'][0]))\n","metric = load_metric(\"accuracy\")\n","metric1= load_metric(\"f1\")\n","# evaluation function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return {\"acc\": metric.compute(predictions=predictions, references=labels), \"f1\":metric1.compute(predictions=predictions, references=labels, average='macro')}\n","#load model\n","model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n","trainer = Trainer(\n","  model=model,\n","  train_dataset=tokenized_ds['train'],\n","  eval_dataset=tokenized_ds['val'],\n","  compute_metrics=compute_metrics,\n","  tokenizer=tokenizer,\n","  data_collator=data_collator\n","  )\n","#evaluation\n","print(\"Prediction on validation data:\")\n","print(trainer.predict(tokenized_ds['val'])[-1])\n","print(\"Prediction on test data:\")\n","print(trainer.predict(tokenized_ds['test'])[-1])\n","#saving to DF\n","val_predictions = np.argmax(trainer.predict(tokenized_ds['val'])[0], axis=-1)\n","test_predictions = np.argmax(trainer.predict(tokenized_ds['test'])[0], axis=-1)\n","val_data_frame=pd.DataFrame({'tweet': ds['val']['tweet'], 'true_label': ds['val']['label'], 'pred_label': val_predictions})\n","test_data_frame=pd.DataFrame({'tweet': ds['test']['tweet'], 'true_label': ds['test']['label'], 'pred_label': test_predictions})\n","print(\"Validation classification report:\")\n","print(classification_report(val_data_frame['true_label'], val_data_frame['pred_label']))\n","print(\"Test classification report:\")\n","print(classification_report(test_data_frame['true_label'], test_data_frame['pred_label']))\n","test_data_frame.to_csv('/content/gdrive/MyDrive/lfd-proj/models-output/bert-base-cased-fintuned-1.csv', index=False)\n","val_data_frame.to_csv('/content/gdrive/MyDrive/lfd-proj/val-models-output/bert-base-cased-fintuned-1.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"OEEUARpOmPhk","executionInfo":{"status":"ok","timestamp":1667130417442,"user_tz":-60,"elapsed":136841,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"a93ad07c-393e-4d28-d440-b9cc368103e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-1/snapshots/9e714e379af2718b93532be9eef430816f41affb/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-1/snapshots/9e714e379af2718b93532be9eef430816f41affb/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-1/snapshots/9e714e379af2718b93532be9eef430816f41affb/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-1/snapshots/9e714e379af2718b93532be9eef430816f41affb/tokenizer_config.json\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-083e98e21ff146fe.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8089ff226fb8731b.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-41fbe0df2e8910a5.arrow\n"]},{"output_type":"stream","name":"stdout","text":["512\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-1/snapshots/9e714e379af2718b93532be9eef430816f41affb/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sara-nabhani/bert-base-cased-fintuned-1\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"OFF\",\n","    \"1\": \"NOT\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"NOT\": 1,\n","    \"OFF\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.23.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-1/snapshots/9e714e379af2718b93532be9eef430816f41affb/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sara-nabhani/bert-base-cased-fintuned-1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1000\n","  Batch size = 8\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["Prediction on validation data:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 860\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["{'test_loss': 0.4948499798774719, 'test_acc': {'accuracy': 0.795}, 'test_f1': {'f1': 0.7733602058110444}, 'test_runtime': 35.0764, 'test_samples_per_second': 28.509, 'test_steps_per_second': 3.564}\n","Prediction on test data:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1000\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["{'test_loss': 0.37620607018470764, 'test_acc': {'accuracy': 0.8383720930232558}, 'test_f1': {'f1': 0.790530228026614}, 'test_runtime': 29.9926, 'test_samples_per_second': 28.674, 'test_steps_per_second': 3.601}\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 860\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation classification report:\n","              precision    recall  f1-score   support\n","\n","           0       0.72      0.69      0.70       352\n","           1       0.84      0.85      0.84       648\n","\n","    accuracy                           0.80      1000\n","   macro avg       0.78      0.77      0.77      1000\n","weighted avg       0.79      0.80      0.79      1000\n","\n","Test classification report:\n","              precision    recall  f1-score   support\n","\n","           0       0.74      0.65      0.69       240\n","           1       0.87      0.91      0.89       620\n","\n","    accuracy                           0.84       860\n","   macro avg       0.81      0.78      0.79       860\n","weighted avg       0.83      0.84      0.83       860\n","\n"]}]},{"cell_type":"markdown","source":["# bert-base-cased-finetuned-2 Model Evaluation"],"metadata":{"id":"SSq6w14hnUnN"}},{"cell_type":"code","source":["model_id = 'sara-nabhani/bert-base-cased-fintuned-2'\n","#load tokenizer and tokenize dataset\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples['tweet'], padding='max_length', truncation=True, max_length=100)\n","\n","tokenized_ds = ds.map(tokenize_function, batched=True)\n","\n","print(len(tokenized_ds['train']['input_ids'][0]))\n","metric = load_metric(\"accuracy\")\n","metric1= load_metric(\"f1\")\n","# evaluation function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return {\"acc\": metric.compute(predictions=predictions, references=labels), \"f1\":metric1.compute(predictions=predictions, references=labels, average='macro')}\n","#load model\n","model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n","trainer = Trainer(\n","  model=model,\n","  train_dataset=tokenized_ds['train'],\n","  eval_dataset=tokenized_ds['val'],\n","  compute_metrics=compute_metrics,\n","  tokenizer=tokenizer,\n","  data_collator=data_collator\n","  )\n","#evaluation\n","print(\"Prediction on validation data:\")\n","print(trainer.predict(tokenized_ds['val'])[-1])\n","print(\"Prediction on test data:\")\n","print(trainer.predict(tokenized_ds['test'])[-1])\n","#saving to DF\n","val_predictions = np.argmax(trainer.predict(tokenized_ds['val'])[0], axis=-1)\n","test_predictions = np.argmax(trainer.predict(tokenized_ds['test'])[0], axis=-1)\n","val_data_frame=pd.DataFrame({'tweet': ds['val']['tweet'], 'true_label': ds['val']['label'], 'pred_label': val_predictions})\n","test_data_frame=pd.DataFrame({'tweet': ds['test']['tweet'], 'true_label': ds['test']['label'], 'pred_label': test_predictions})\n","print(\"Validation classification report:\")\n","print(classification_report(val_data_frame['true_label'], val_data_frame['pred_label']))\n","print(\"Test classification report:\")\n","print(classification_report(test_data_frame['true_label'], test_data_frame['pred_label']))\n","test_data_frame.to_csv('/content/gdrive/MyDrive/lfd-proj/models-output/bert-base-cased-fintuned-2.csv', index=False)\n","val_data_frame.to_csv('/content/gdrive/MyDrive/lfd-proj/val-models-output/bert-base-cased-fintuned-2.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"AOFtBTSpnZD_","executionInfo":{"status":"ok","timestamp":1667130446994,"user_tz":-60,"elapsed":29571,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"e3dd8158-56d8-4e0b-fde6-d9e320f585fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-2/snapshots/cffafc001441d16b2d3bb34ecd64cfbc3ed168c5/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-2/snapshots/cffafc001441d16b2d3bb34ecd64cfbc3ed168c5/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-2/snapshots/cffafc001441d16b2d3bb34ecd64cfbc3ed168c5/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-2/snapshots/cffafc001441d16b2d3bb34ecd64cfbc3ed168c5/tokenizer_config.json\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-217f6bc347d81b47.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22a0d9f2782b6985.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-887477a56186b157.arrow\n"]},{"output_type":"stream","name":"stdout","text":["100\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-2/snapshots/cffafc001441d16b2d3bb34ecd64cfbc3ed168c5/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sara-nabhani/bert-base-cased-fintuned-2\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"OFF\",\n","    \"1\": \"NOT\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"NOT\": 1,\n","    \"OFF\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.23.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-2/snapshots/cffafc001441d16b2d3bb34ecd64cfbc3ed168c5/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sara-nabhani/bert-base-cased-fintuned-2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1000\n","  Batch size = 8\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["Prediction on validation data:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 860\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["{'test_loss': 0.4887142479419708, 'test_acc': {'accuracy': 0.794}, 'test_f1': {'f1': 0.7730237641915239}, 'test_runtime': 6.3006, 'test_samples_per_second': 158.716, 'test_steps_per_second': 19.839}\n","Prediction on test data:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1000\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["{'test_loss': 0.37155917286872864, 'test_acc': {'accuracy': 0.8395348837209302}, 'test_f1': {'f1': 0.7911313542319309}, 'test_runtime': 5.4414, 'test_samples_per_second': 158.049, 'test_steps_per_second': 19.848}\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 860\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation classification report:\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.70      0.70       352\n","           1       0.84      0.85      0.84       648\n","\n","    accuracy                           0.79      1000\n","   macro avg       0.77      0.77      0.77      1000\n","weighted avg       0.79      0.79      0.79      1000\n","\n","Test classification report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.64      0.69       240\n","           1       0.87      0.92      0.89       620\n","\n","    accuracy                           0.84       860\n","   macro avg       0.81      0.78      0.79       860\n","weighted avg       0.83      0.84      0.84       860\n","\n"]}]},{"cell_type":"markdown","source":["# bert-base-cased-finetuned-3 Model Evaluation"],"metadata":{"id":"RFGC8K01rOxp"}},{"cell_type":"code","source":["model_id = 'sara-nabhani/bert-base-cased-fintuned-3'\n","#load tokenizer and tokenize dataset\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples['tweet'], padding='max_length', truncation=True, max_length=200)\n","\n","tokenized_ds = ds.map(tokenize_function, batched=True)\n","\n","print(len(tokenized_ds['train']['input_ids'][0]))\n","metric = load_metric(\"accuracy\")\n","metric1= load_metric(\"f1\")\n","# evaluation function\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return {\"acc\": metric.compute(predictions=predictions, references=labels), \"f1\":metric1.compute(predictions=predictions, references=labels, average='macro')}\n","#load model\n","model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n","trainer = Trainer(\n","  model=model,\n","  train_dataset=tokenized_ds['train'],\n","  eval_dataset=tokenized_ds['val'],\n","  compute_metrics=compute_metrics,\n","  tokenizer=tokenizer,\n","  data_collator=data_collator\n","  )\n","#evaluation\n","print(\"Prediction on validation data:\")\n","print(trainer.predict(tokenized_ds['val'])[-1])\n","print(\"Prediction on test data:\")\n","print(trainer.predict(tokenized_ds['test'])[-1])\n","#saving to DF\n","val_predictions = np.argmax(trainer.predict(tokenized_ds['val'])[0], axis=-1)\n","test_predictions = np.argmax(trainer.predict(tokenized_ds['test'])[0], axis=-1)\n","val_data_frame=pd.DataFrame({'tweet': ds['val']['tweet'], 'true_label': ds['val']['label'], 'pred_label': val_predictions})\n","test_data_frame=pd.DataFrame({'tweet': ds['test']['tweet'], 'true_label': ds['test']['label'], 'pred_label': test_predictions})\n","print(\"Validation classification report:\")\n","print(classification_report(val_data_frame['true_label'], val_data_frame['pred_label']))\n","print(\"Test classification report:\")\n","print(classification_report(test_data_frame['true_label'], test_data_frame['pred_label']))\n","test_data_frame.to_csv('/content/gdrive/MyDrive/lfd-proj/models-output/bert-base-cased-fintuned-3.csv', index=False)\n","val_data_frame.to_csv('/content/gdrive/MyDrive/lfd-proj/val-models-output/bert-base-cased-fintuned-3.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"teYiZ9aKrP5b","executionInfo":{"status":"ok","timestamp":1667130501280,"user_tz":-60,"elapsed":54302,"user":{"displayName":"Sara Nabhani","userId":"14242704810617135500"}},"outputId":"24822774-1e45-4a06-ed70-84eea009655d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-3/snapshots/834db08c182de1e2364035e67ee568649ad5f7c9/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-3/snapshots/834db08c182de1e2364035e67ee568649ad5f7c9/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-3/snapshots/834db08c182de1e2364035e67ee568649ad5f7c9/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-3/snapshots/834db08c182de1e2364035e67ee568649ad5f7c9/tokenizer_config.json\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8426729844706924.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1d009978f9957eae.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/sara-nabhani___csv/sara-nabhani--lfd-proj-a35fbd5b9bbbc3d0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-79a34a85d6570240.arrow\n"]},{"output_type":"stream","name":"stdout","text":["200\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-3/snapshots/834db08c182de1e2364035e67ee568649ad5f7c9/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"sara-nabhani/bert-base-cased-fintuned-3\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"OFF\",\n","    \"1\": \"NOT\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"NOT\": 1,\n","    \"OFF\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.23.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sara-nabhani--bert-base-cased-fintuned-3/snapshots/834db08c182de1e2364035e67ee568649ad5f7c9/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at sara-nabhani/bert-base-cased-fintuned-3.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1000\n","  Batch size = 8\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["Prediction on validation data:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 860\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["{'test_loss': 0.49006927013397217, 'test_acc': {'accuracy': 0.788}, 'test_f1': {'f1': 0.765781053896873}, 'test_runtime': 12.7905, 'test_samples_per_second': 78.183, 'test_steps_per_second': 9.773}\n","Prediction on test data:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1000\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["{'test_loss': 0.3795793652534485, 'test_acc': {'accuracy': 0.8395348837209302}, 'test_f1': {'f1': 0.7899007923862935}, 'test_runtime': 11.0566, 'test_samples_per_second': 77.782, 'test_steps_per_second': 9.768}\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet. If tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 860\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation classification report:\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.68      0.69       352\n","           1       0.83      0.85      0.84       648\n","\n","    accuracy                           0.79      1000\n","   macro avg       0.77      0.76      0.77      1000\n","weighted avg       0.79      0.79      0.79      1000\n","\n","Test classification report:\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.63      0.69       240\n","           1       0.87      0.92      0.89       620\n","\n","    accuracy                           0.84       860\n","   macro avg       0.81      0.78      0.79       860\n","weighted avg       0.83      0.84      0.84       860\n","\n"]}]}]}